{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0307b025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import nflreadpy as nfl\n",
    "import nfl_data_py as nfl_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c52f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NFL data for 2025 season\n",
    "season = 2025\n",
    "schedule = nfl.load_schedules(seasons=[season])\n",
    "schedule = schedule.to_pandas()\n",
    "\n",
    "# Select only regular season games\n",
    "team_stats = nfl.load_team_stats([season, summary_level='reg'])\n",
    "team_stats = team_stats.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2660fe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for EPA per play\n",
    "team_stats[\"pass_plays\"] = team_stats[\"attempts\"] + team_stats[\"sacks_suffered\"]\n",
    "team_stats[\"rush_plays\"] = team_stats[\"carries\"]\n",
    "team_stats[\"total_plays\"] = team_stats[\"pass_plays\"] + team_stats[\"rush_plays\"]\n",
    "team_stats[\"off_epa_per_play\"] = (\n",
    "    (team_stats[\"passing_epa\"] * team_stats[\"pass_plays\"]) +\n",
    "    (team_stats[\"rushing_epa\"] * team_stats[\"rush_plays\"])\n",
    ") / team_stats[\"total_plays\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f15814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns points for and against to team_stats\n",
    "home = schedule[[\"season\", \"week\", \"home_team\", \"away_team\", \"home_score\", \"away_score\"]].copy()\n",
    "home[\"team\"] = home[\"home_team\"]\n",
    "home[\"points_for\"] = home[\"home_score\"]\n",
    "home[\"points_against\"] = home[\"away_score\"]\n",
    "\n",
    "away = schedule[[\"season\", \"week\", \"home_team\", \"away_team\", \"home_score\", \"away_score\"]].copy()\n",
    "away[\"team\"] = away[\"away_team\"]\n",
    "away[\"points_for\"] = away[\"away_score\"]\n",
    "away[\"points_against\"] = away[\"home_score\"]\n",
    "\n",
    "schedule_long = pd.concat([home, away], ignore_index=True)\n",
    "\n",
    "# Merge into team_stats\n",
    "team_stats = team_stats.merge(\n",
    "    schedule_long[[\"season\", \"week\", \"team\", \"points_for\", \"points_against\"]],\n",
    "    on=[\"season\", \"week\", \"team\"],\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85cb15f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for turnover differential\n",
    "# Offensive giveaways\n",
    "team_stats[\"giveaways\"] = (\n",
    "    team_stats[\"passing_interceptions\"] + \n",
    "    team_stats[\"rushing_fumbles_lost\"] + \n",
    "    team_stats[\"receiving_fumbles_lost\"] + \n",
    "    team_stats[\"sack_fumbles_lost\"]\n",
    ")\n",
    "\n",
    "# Defensive takeaways\n",
    "team_stats[\"takeaways\"] = (\n",
    "    team_stats[\"def_interceptions\"] + \n",
    "    team_stats[\"fumble_recovery_opp\"]   # opp fumbles recovered by your team\n",
    ")\n",
    "\n",
    "# Turnover differential\n",
    "team_stats[\"turnover_diff\"] = team_stats[\"takeaways\"] - team_stats[\"giveaways\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a5601d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web scrape defensive EPA/play from https://sumersports.com/teams/defensive/\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://sumersports.com/teams/defensive/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Locate table\n",
    "table = soup.find('table')\n",
    "\n",
    "headers = [th.get_text(strip=True) for th in table.find_all('th')]\n",
    "\n",
    "# Extract rows\n",
    "rows = []\n",
    "for tr in table.find_all('tr')[1:]:  # Skip header row\n",
    "    cols = [td.get_text(strip=True) for td in tr.find_all('td')]\n",
    "    if cols:  # Check row is not empty\n",
    "        rows.append(cols)\n",
    "\n",
    "df = pd.DataFrame(rows, columns=headers)\n",
    "# Clean up team names (remove extra spaces)\n",
    "df['team'] = df['Team'].str.replace(r'^\\d+\\.\\s*', '', regex=True)\n",
    "df['EPA/Play'] = pd.to_numeric(df['EPA/Play'], errors='coerce')\n",
    "\n",
    "def_epa_df = df[['team', 'EPA/Play']].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "30af6c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web scrape 3rd down conversion % (offense and defense) from https://www.teamrankings.com/\n",
    "urls = {\n",
    "    'third_down': 'https://www.teamrankings.com/nfl/stat/third-down-conversion-pct',\n",
    "    'opponent_third_down': 'https://www.teamrankings.com/nfl/stat/opponent-third-down-conversion-pct'\n",
    "}\n",
    "\n",
    "# Function to scrape data from a given URL\n",
    "def scrape_data(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Locate table\n",
    "    table = soup.find('table')\n",
    "\n",
    "    # Extract headers\n",
    "    headers = [th.get_text(strip=True) for th in table.find_all('th')]\n",
    "\n",
    "    # Extract rows\n",
    "    rows = []\n",
    "    for tr in table.find_all('tr')[1:]:  # Skip header\n",
    "        cols = [td.get_text(strip=True) for td in tr.find_all('td')]\n",
    "        if cols:  # Check row is not empty\n",
    "            rows.append(cols)\n",
    "\n",
    "    # Create a df\n",
    "    df = pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "    # Clean up team names (remove extra spaces)\n",
    "    df['Team'] = df['Team'].str.strip()\n",
    "\n",
    "    return df[['Team', '2025']]\n",
    "\n",
    "# Scrape data\n",
    "third_down_df = scrape_data(urls['third_down'])\n",
    "opponent_third_down_df = scrape_data(urls['opponent_third_down'])\n",
    "\n",
    "# Merge on the 'Team' column\n",
    "merged_third_down_df = pd.merge(third_down_df, opponent_third_down_df, on='Team', suffixes=('_Offense', '_Defense'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e1bfcc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add indicator column for winner\n",
    "team_stats['winner'] = (team_stats['points_for'] > team_stats['points_against']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "33c33799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with cumulative averages of features\n",
    "def cumulative_avg(df, up_to_week):\n",
    "    stats_cols = [\"off_epa_per_play\", \"points_for\", \"points_against\", \"turnover_diff\"]\n",
    "\n",
    "    # filter games up to target week\n",
    "    df_filtered = df[df[\"week\"] <= up_to_week].copy()\n",
    "\n",
    "    # calculate cumulative average per team\n",
    "    cumu = (\n",
    "        df_filtered.groupby(\"team\")[stats_cols]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # rename columns\n",
    "    cumu = cumu.rename(columns={col: f\"{col}\" for col in stats_cols})\n",
    "\n",
    "    return cumu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "51f5c054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cumulative avg dataframe for weeks 1-3\n",
    "cumulative_df = cumulative_avg(team_stats, up_to_week=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "95a686ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all team names to abbreviations\n",
    "TEAM_ABBR = {\n",
    "    \"ARI\": [\"Arizona Cardinals\", \"Arizona\"],\n",
    "    \"ATL\": [\"Atlanta Falcons\", \"Atlanta\"],\n",
    "    \"BAL\": [\"Baltimore Ravens\", \"Baltimore\"],\n",
    "    \"BUF\": [\"Buffalo Bills\", \"Buffalo\"],\n",
    "    \"CAR\": [\"Carolina Panthers\", \"Carolina\"],\n",
    "    \"CHI\": [\"Chicago Bears\", \"Chicago\"],\n",
    "    \"CIN\": [\"Cincinnati Bengals\", \"Cincinnati\"],\n",
    "    \"CLE\": [\"Cleveland Browns\", \"Cleveland\"],\n",
    "    \"DAL\": [\"Dallas Cowboys\", \"Dallas\"],\n",
    "    \"DEN\": [\"Denver Broncos\", \"Denver\"],\n",
    "    \"DET\": [\"Detroit Lions\", \"Detroit\"],\n",
    "    \"GB\":  [\"Green Bay Packers\", \"Green Bay\"],\n",
    "    \"HOU\": [\"Houston Texans\", \"Houston\"],\n",
    "    \"IND\": [\"Indianapolis Colts\", \"Indianapolis\"],\n",
    "    \"JAX\": [\"Jacksonville Jaguars\", \"Jacksonville\"],\n",
    "    \"KC\":  [\"Kansas City Chiefs\", \"Kansas City\"],\n",
    "    \"LV\":  [\"Las Vegas Raiders\", \"Las Vegas\"],\n",
    "    \"LAC\": [\"Los Angeles Chargers\", \"LA Chargers\"],\n",
    "    \"LAR\": [\"Los Angeles Rams\", \"LA Rams\"],\n",
    "    \"MIA\": [\"Miami Dolphins\", \"Miami\"],\n",
    "    \"MIN\": [\"Minnesota Vikings\", \"Minnesota\"],\n",
    "    \"NE\":  [\"New England Patriots\", \"New England\"],\n",
    "    \"NO\":  [\"New Orleans Saints\", \"New Orleans\"],\n",
    "    \"NYG\": [\"New York Giants\", \"NY Giants\"],\n",
    "    \"NYJ\": [\"New York Jets\", \"NY Jets\"],\n",
    "    \"PHI\": [\"Philadelphia Eagles\", \"Philadelphia\"],\n",
    "    \"PIT\": [\"Pittsburgh Steelers\", \"Pittsburgh\"],\n",
    "    \"SF\":  [\"San Francisco 49ers\", \"San Francisco\"],\n",
    "    \"SEA\": [\"Seattle Seahawks\", \"Seattle\"],\n",
    "    \"TB\":  [\"Tampa Bay Buccaneers\", \"Tampa Bay\"],\n",
    "    \"TEN\": [\"Tennessee Titans\", \"Tennessee\"],\n",
    "    \"WAS\": [\"Washington Commanders\", \"Washington\"],\n",
    "}\n",
    "\n",
    "# flatten into reverse lookup once\n",
    "NAME_TO_ABBR = {name: abbr for abbr, names in TEAM_ABBR.items() for name in names}\n",
    "\n",
    "# standardize team names in scraped dfs\n",
    "def_epa_df[\"team\"] = def_epa_df[\"team\"].map(NAME_TO_ABBR)\n",
    "merged_third_down_df[\"team\"] = merged_third_down_df[\"Team\"].map(NAME_TO_ABBR)\n",
    "merged_third_down_df.drop(columns=[\"Team\"], inplace=True)\n",
    "def_epa_df = def_epa_df.rename(columns={\"EPA/Play\": \"def_epa_per_play\"})\n",
    "merged_third_down_df = merged_third_down_df.rename(columns={\n",
    "    \"2025_Offense\": \"off_third_down_pct\",\n",
    "    \"2025_Defense\": \"def_third_down_pct\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "13b11117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all three DataFrames \n",
    "all_stats = cumulative_df.merge(def_epa_df, on='team', how=\"inner\").merge(merged_third_down_df, on=\"team\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "aebe2d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  team  off_epa_per_play  points_for  points_against  turnover_diff  \\\n",
      "0  ARI          1.407971   20.666667       17.000000       0.666667   \n",
      "1  ATL         -2.770102   14.000000       19.666667       0.333333   \n",
      "2  BAL          6.213836   37.000000       32.000000       0.000000   \n",
      "3  BUF          7.803691   34.000000       23.666667       1.000000   \n",
      "4  CAR         -2.611105   20.666667       17.666667       0.000000   \n",
      "5  CHI          0.191999   25.333333       31.000000       1.000000   \n",
      "6  CIN         -5.970140   19.333333       30.333333      -1.333333   \n",
      "7  CLE         -7.418851   15.333333       22.666667      -1.333333   \n",
      "8  DAL          2.157078   24.666667       30.666667      -1.666667   \n",
      "9  DEN         -0.643781   22.666667       21.333333       0.000000   \n",
      "\n",
      "   def_epa_per_play off_third_down_pct def_third_down_pct  \n",
      "0             -0.01             38.46%             34.62%  \n",
      "1             -0.05             41.82%             34.09%  \n",
      "2              0.14             40.91%             41.94%  \n",
      "3              0.02             42.00%             42.00%  \n",
      "4              0.04             38.18%             37.21%  \n",
      "5              0.00             45.45%             29.27%  \n",
      "6              0.10             36.00%             49.09%  \n",
      "7             -0.05             37.70%             44.83%  \n",
      "8              0.25             42.55%             58.18%  \n",
      "9             -0.11             39.22%             32.73%  \n"
     ]
    }
   ],
   "source": [
    "print(all_stats.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nflmodel)",
   "language": "python",
   "name": "nflmodel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
